{"cells":[{"cell_type":"markdown","metadata":{"id":"Q3fIJM8ebasA"},"source":["# Fine Tuning GPT-3.5-Turbo\n","\n","In this notebook, we walk through an example of fine-tuning gpt-3.5-turbo.\n","\n","Specifically, we attempt to distill GPT-4's knowledge, by generating training data with GPT-4 to then fine-tune GPT-3.5.\n","\n","All training data is generated using two different sections of our index data, creating both a training and evalution set.\n","\n","Evaluation is done using the `ragas` library, which we will detail later on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QdMJRtqbasD","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1693018764701,"user_tz":420,"elapsed":21884,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}},"outputId":"543ab119-9118-4e0f-955e-c4bc8090eebf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-index\n","  Downloading llama_index-0.8.10-py3-none-any.whl (706 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/706.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m696.3/706.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.5/706.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pypdf\n","  Downloading pypdf-3.15.2-py3-none-any.whl (271 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/271.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.1/271.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ragas\n","  Downloading ragas-0.0.11-py3-none-any.whl (31 kB)\n","Collecting tiktoken (from llama-index)\n","  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dataclasses-json (from llama-index)\n","  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n","Collecting langchain>=0.0.262 (from llama-index)\n","  Downloading langchain-0.0.273-py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sqlalchemy>=2.0.15 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n","Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n","Collecting openai>=0.26.4 (from llama-index)\n","  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n","Collecting urllib3<2 (from llama-index)\n","  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n","Collecting typing-inspect>=0.8.0 (from llama-index)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.7.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.11.2)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.7)\n","Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n","  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Collecting sentencepiece (from sentence-transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from ragas)\n","  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting protobuf<=3.20.0 (from ragas)\n","  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic<2.0 (from ragas)\n","  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (3.8.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (4.0.3)\n","Collecting langsmith<0.1.0,>=0.0.21 (from langchain>=0.0.262->llama-index)\n","  Downloading langsmith-0.0.26-py3-none-any.whl (34 kB)\n","Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (2.8.5)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama-index) (2.0.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->llama-index) (2.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets->ragas)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets->ragas)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets->ragas)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=79a432055f7323dcaad6aea620b2a4f33e3029a403c5d20d18e15807e7148da4\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence-transformers\n","Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, urllib3, pypdf, pydantic, protobuf, mypy-extensions, marshmallow, dill, typing-inspect, multiprocess, tiktoken, openai, langsmith, huggingface-hub, dataclasses-json, transformers, langchain, datasets, llama-index, sentence-transformers, ragas\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.4\n","    Uninstalling urllib3-2.0.4:\n","      Successfully uninstalled urllib3-2.0.4\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.2.1\n","    Uninstalling pydantic-2.2.1:\n","      Successfully uninstalled pydantic-2.2.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-functions 1.13.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","googleapis-common-protos 1.60.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n","tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dataclasses-json-0.5.14 datasets-2.14.4 dill-0.3.7 huggingface-hub-0.16.4 langchain-0.0.273 langsmith-0.0.26 llama-index-0.8.10 marshmallow-3.20.1 multiprocess-0.70.15 mypy-extensions-1.0.0 openai-0.27.9 protobuf-3.20.0 pydantic-1.10.12 pypdf-3.15.2 ragas-0.0.11 safetensors-0.3.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.32.0 typing-inspect-0.9.0 urllib3-1.26.16 xxhash-3.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}],"source":["!pip install llama-index pypdf sentence-transformers ragas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHpR_evRbasE"},"outputs":[],"source":["import os\n","import openai\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"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\"\n","openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]},{"cell_type":"markdown","metadata":{"id":"rOw2gMDubasE"},"source":["## Data Setup\n","\n","Here, we first down load the PDF that we will use to generate training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZeplTKibasF","outputId":"c5140b11-83f2-4e7d-c4e4-a651a50ed713","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693008298730,"user_tz":420,"elapsed":534,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 20.7M  100 20.7M    0     0  60.3M      0 --:--:-- --:--:-- --:--:-- 60.4M\n"]}],"source":["#!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"]},{"cell_type":"markdown","metadata":{"id":"aSz-0Es5basF"},"source":["The next step is generating a training and eval dataset.\n","\n","We will generate 40 questions on different sections of the PDF we downloaded.\n","\n","We can use GPT-3.5 on the eval questions to get our baseline performance.\n","\n","Then, we will use GPT-4 on the train questions to generate our training data. The training data will be collected with out `OpenAIFineTuningHandler`.\n","\n","This step is entirely optional if you don't want to spend the time/tokens -- the eval and training questions are also provided in this folder, as well as the training data!"]},{"cell_type":"markdown","metadata":{"id":"09r5X8TYbasG"},"source":["### Train Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pKTsTzwbasG"},"outputs":[],"source":["from llama_index import SimpleDirectoryReader, ServiceContext\n","from llama_index.llms import OpenAI\n","from llama_index.evaluation import DatasetGenerator\n","\n","documents = SimpleDirectoryReader(\n","    input_files=[\"Cat-CKD.txt\"] #\"IPCC_AR6_WGII_Chapter03.pdf\",\"IHI_report_en.pdf\"\n",").load_data()\n","\n","# Shuffle the documents\n","import random\n","\n","random.seed(42)\n","random.shuffle(documents)\n","\n","gpt_35_context = ServiceContext.from_defaults(\n","    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Icni9CGqbasH"},"outputs":[],"source":["question_gen_query = (\n","    \"You are a Teacher/ Professor. Your task is to setup \"\n","    \"a quiz/examination. Using the provided context from a \"\n","    \"report on climate change and the oceans, formulate \"\n","    \"a single question that captures an important fact from the \"\n","    \"context. Restrict the question to the context information provided.\"\n",")\n","\n","dataset_generator = DatasetGenerator.from_documents(\n","    documents[:50],\n","    question_gen_query=question_gen_query,\n","    service_context=gpt_35_context,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9s_4vqSbasH","outputId":"09b2cb04-47b4-45e2-b757-401f2ffbca32","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693019688868,"user_tz":420,"elapsed":12458,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated  15  questions\n"]}],"source":["# NOTE: this may take some time. Go grab a coffee!\n","questions = dataset_generator.generate_questions_from_nodes(num=40)\n","print(\"Generated \", len(questions), \" questions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hh_geSX8basH"},"outputs":[],"source":["with open(\"train_questions.txt\", \"w\") as f:\n","    for question in questions:\n","        f.write(question + \"\\n\")"]},{"cell_type":"code","source":["len(documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z96Eli1M1sV-","executionInfo":{"status":"ok","timestamp":1693019724927,"user_tz":420,"elapsed":6,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}},"outputId":"702f7b73-5fad-4a15-ee2d-88e40dcfd396"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"61mF7guLbasH"},"source":["### Eval Generation\n","\n","Now, lets generate questions on a completely different set of documents, in order to create our eval dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivceNT09basH","executionInfo":{"status":"ok","timestamp":1693019767340,"user_tz":420,"elapsed":213,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}},"colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"047ce600-15a4-48a1-a50d-17fc1ad8fcd5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'dataset_generator = DatasetGenerator.from_documents(\\n    documents[\\n        50:\\n    ],  # since we generated ~1 question for 40 documents, we can skip the first 40\\n    question_gen_query=question_gen_query,\\n    service_context=gpt_35_context,\\n)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":50}],"source":["dataset_generator = DatasetGenerator.from_documents(\n","    documents, # since we generated ~1 question for 40 documents, we can skip the first 40\n","    question_gen_query=question_gen_query,\n","    service_context=gpt_35_context,\n",")\n","\n","'''dataset_generator = DatasetGenerator.from_documents(\n","    documents[\n","        50:\n","    ],  # since we generated ~1 question for 40 documents, we can skip the first 40\n","    question_gen_query=question_gen_query,\n","    service_context=gpt_35_context,\n",")'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSAEy5wpbasI","outputId":"fc6dcd16-a3ea-4f24-fbd3-4f3b7ab555f9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693019779850,"user_tz":420,"elapsed":9804,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated  15  questions\n"]}],"source":["# NOTE: this may take some time. Go grab a coffee!\n","questions = dataset_generator.generate_questions_from_nodes(num=40)\n","print(\"Generated \", len(questions), \" questions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUGBJyc7basI"},"outputs":[],"source":["with open(\"eval_questions.txt\", \"w\") as f:\n","    for question in questions:\n","        f.write(question + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"JDSd7AIybasI"},"source":["## Initial Eval with GPT-3.5-Turbo Query Engine\n","\n","For this eval, we will be using the [`ragas` evaluation library](https://github.com/explodinggradients/ragas).\n","\n","Ragas has a ton of evaluation metrics for RAG pipelines, and you can read about them [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md).\n","\n","For this notebook, we will be using the following two metrics\n","\n","- `answer_relevancy` - This measures how relevant is the generated answer to the prompt. If the generated answer is incomplete or contains redundant information the score will be low. This is quantified by working out the chance of an LLM generating the given question using the generated answer. Values range (0,1), higher the better.\n","- `faithfulness` - This measures the factual consistency of the generated answer against the given context. This is done using a multi step paradigm that includes creation of statements from the generated answer followed by verifying each of these statements against the context. The answer is scaled to (0,1) range. Higher the better."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cy-X-JhrbasI"},"outputs":[],"source":["questions = []\n","with open(\"eval_questions.txt\", \"r\") as f:\n","    for line in f:\n","        questions.append(line.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8ZFQr_sbasI"},"outputs":[],"source":["from llama_index import VectorStoreIndex\n","\n","# limit the context window to 2048 tokens so that refine is used\n","gpt_35_context = ServiceContext.from_defaults(\n","    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3), context_window=2048\n",")\n","\n","index = VectorStoreIndex.from_documents(documents, service_context=gpt_35_context)\n","\n","query_engine = index.as_query_engine(similarity_top_k=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z58fuJQ_basI"},"outputs":[],"source":["contexts = []\n","answers = []\n","\n","for question in questions:\n","    response = query_engine.query(question)\n","    contexts.append([x.node.get_content() for x in response.source_nodes])\n","    answers.append(str(response))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0TiVi6cbasI","outputId":"07851e77-fccc-4cd5-b097-8083dcc6f6ca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693020187171,"user_tz":420,"elapsed":302519,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["evaluating with [answer_relevancy]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:28<00:00, 28.27s/it]\n"]},{"output_type":"stream","name":"stdout","text":["evaluating with [faithfulness]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [04:34<00:00, 274.03s/it]\n"]},{"output_type":"stream","name":"stdout","text":["{'ragas_score': 0.9340, 'answer_relevancy': 0.9908, 'faithfulness': 0.8833}\n"]}],"source":["from datasets import Dataset\n","from ragas import evaluate\n","from ragas.metrics import answer_relevancy, faithfulness\n","\n","ds = Dataset.from_dict(\n","    {\n","        \"question\": questions,\n","        \"answer\": answers,\n","        \"contexts\": contexts,\n","    }\n",")\n","\n","result = evaluate(ds, [answer_relevancy, faithfulness])\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"Q_Xb9m1dbasI"},"source":["## GPT-4 to Collect Training Data\n","\n","Here, we use GPT-4 and the `OpenAIFineTuningHandler` to collect data that we want to train on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W68qggawbasI"},"outputs":[],"source":["from llama_index import ServiceContext\n","from llama_index.llms import OpenAI\n","from llama_index.callbacks import OpenAIFineTuningHandler\n","from llama_index.callbacks import CallbackManager\n","\n","finetuning_handler = OpenAIFineTuningHandler()\n","callback_manager = CallbackManager([finetuning_handler])\n","\n","gpt_4_context = ServiceContext.from_defaults(\n","    llm=OpenAI(model=\"gpt-4\", temperature=0.3),\n","    context_window=2048,  # limit the context window artifically to test refine process\n","    callback_manager=callback_manager,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMsapDGjbasI"},"outputs":[],"source":["questions = []\n","with open(\"train_questions.txt\", \"r\") as f:\n","    for line in f:\n","        questions.append(line.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7AWjp5UbasI"},"outputs":[],"source":["from llama_index import VectorStoreIndex\n","\n","index = VectorStoreIndex.from_documents(documents, service_context=gpt_4_context)\n","\n","query_engine = index.as_query_engine(similarity_top_k=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXRazK3obasJ"},"outputs":[],"source":["for question in questions:\n","    response = query_engine.query(question)"]},{"cell_type":"markdown","metadata":{"id":"PghSrk_rbasJ"},"source":["## Create Fine-Tuning Data\n","\n","Fine-Tuning data must be written as a list of messages in a `.jsonl` file. Using the finetuning-handler, we can easily write the messages to a `.jsonl` file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6suUlT5basJ","outputId":"54b0c5ea-bb73-4905-c8e0-5dc68d747331","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693020393834,"user_tz":420,"elapsed":30,"user":{"displayName":"Hiro Ozasa","userId":"13196090229356142516"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote 30 examples to finetuning_cat_CKD.jsonl\n"]}],"source":["finetuning_handler.save_finetuning_events(\"finetuning_cat_CKD.jsonl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxlG1lTubasP"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"llama-index","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo","timestamp":1692993750034},{"file_id":"1RB-h8HwFzkC7ElPyFgG6QdPY3ZXBkQFO","timestamp":1692777952194}]}},"nbformat":4,"nbformat_minor":0}